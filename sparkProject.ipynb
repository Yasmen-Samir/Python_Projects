{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPTUm+AxMmjlrDe20/5HdfX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# pip install pyspark"],"metadata":{"id":"NyPkQ0nJaR8x","executionInfo":{"status":"ok","timestamp":1684749274913,"user_tz":-180,"elapsed":32645,"user":{"displayName":"Yasmen Samir","userId":"08881511615820735826"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5b8278c3-0970-4dd3-fade-98949029dc9f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=a674b35d9e3dca2de9d3e4bfe0ee915325eb104f015b6d36f2d54e14fff8658b\n","  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.4.0\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"VMe-WnSnhAKz","executionInfo":{"status":"ok","timestamp":1684749313770,"user_tz":-180,"elapsed":18452,"user":{"displayName":"Yasmen Samir","userId":"08881511615820735826"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9bb27dd9-567a-4796-9c56-d0a489374b1c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import os\n","from pyspark import SparkConf, SparkContext\n","import time\n","# drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Create a SparkSession\n","SparkContext.getOrCreate().stop()\n","conf = SparkConf().setAppName(\"PageTitleAnalysis\")\n","sc = SparkContext(conf=conf)"],"metadata":{"id":"1rpwT0W3hxGV","executionInfo":{"status":"ok","timestamp":1684749329596,"user_tz":-180,"elapsed":6331,"user":{"displayName":"Yasmen Samir","userId":"08881511615820735826"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["os.chdir(r\"/content/drive/MyDrive/data/\")\n","lines = sc.textFile(\"/content/drive/MyDrive/data/pagecounts-20160101-000000_parsed.out\")"],"metadata":{"id":"8iXPYHMTh1tS","executionInfo":{"status":"ok","timestamp":1684749332035,"user_tz":-180,"elapsed":728,"user":{"displayName":"Yasmen Samir","userId":"08881511615820735826"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# problem 1\n","# Using map reduce\n","\n","start_time = time.time()\n","\n","# Define the map function to extract the page size from each line\n","def extract_page_size(line):\n","    fields = line.split()\n","    if len(fields) >= 4:\n","        try:\n","            page_size = int(fields[3])\n","            return page_size\n","        except ValueError:\n","            pass\n","    return 0\n","\n","# Define the reduce function to find the minimum, maximum, and average page sizes\n","def find_min_max_avg(page_sizes):\n","    min_size = float('inf')\n","    max_size = float('-inf')\n","    total_size = 0\n","    total_count = 0\n","    for size in page_sizes:\n","        if size < min_size:\n","            min_size = size\n","        if size > max_size:\n","            max_size = size\n","        total_size += size\n","        total_count += 1\n","    avg_size = total_size / total_count if total_count > 0 else 0\n","    return (min_size, max_size, avg_size)\n","\n","# Apply the map function to extract the page sizes and group them by a dummy key\n","page_sizes = lines.map(extract_page_size).groupBy(lambda x: 0)\n","\n","# Apply the reduce function to find the minimum, maximum, and average page sizes\n","min_max_avg_sizes = page_sizes.mapValues(find_min_max_avg)\n","\n","# Extract the results from the RDD\n","\n","# Extract the minimum, maximum, and average page sizes from the RDD\n","min_max_avg = min_max_avg_sizes.take(1)[0]\n","\n","for i in range(1000000):\n","    pass\n","\n","# stop measuring the execution time\n","end_time = time.time()\n","\n","# Print the minimum, maximum, and average sizes\n","print(\"using map-reduce: \\n\")\n","print(\"Minimum page size:\", min_max_avg[1][0])\n","print(\"Maximum page size:\", min_max_avg[1][1])\n","print(\"Average page size:\", min_max_avg[1][2])\n","print(\"Time taken:\", end_time - start_time)\n","\n","with open(\"output 1.txt\", \"w\") as f2:\n","  print(\"using map-reduce \\n\\n\", \"Minimum page size (Spark loops):\", min_max_avg[1][0] ,\"\\n\", \"Maximum page size (Spark loops):\", min_max_avg[1][1] ,\"\\n\", \"Average page size (Spark loops):\",  min_max_avg[1][2] ,\"\\n\",\"time taken:\" , end_time - start_time ,\"\\n\", file = f2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xc4rB0x2kDMP","executionInfo":{"status":"ok","timestamp":1684593854191,"user_tz":-180,"elapsed":14339,"user":{"displayName":"Yasmen Samir","userId":"08881511615820735826"}},"outputId":"c10e3f6a-8e75-42f1-9c4b-18a09ef10501"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["using map-reduce: \n","\n","Minimum page size: 0\n","Maximum page size: 141180155987\n","Average page size: 132211.70136297357\n","Time taken: 14.23216438293457\n"]}]},{"cell_type":"code","source":["# problem 1\n","# Using spark loops\n","\n","start_time = time.time()\n","\n","# Compute the min, max, and average page size using Spark loops\n","min_size_spark = float('inf')\n","max_size_spark = float('-inf')\n","total_size_spark = 0\n","total_count_spark = 0\n","for line in lines.toLocalIterator():\n","    fields = line.split()\n","    if len(fields) >= 4:\n","        try:\n","            page_size = int(fields[3])\n","            if page_size < min_size_spark:\n","                min_size_spark = page_size\n","            if page_size > max_size_spark:\n","                max_size_spark = page_size\n","            total_size_spark += page_size\n","            total_count_spark += 1\n","        except ValueError:\n","            pass\n","avg_size_spark = total_size_spark / total_count_spark if total_count_spark > 0 else 0\n","\n","for i in range(1000000):\n","    pass\n","\n","# stop measuring the execution time\n","end_time = time.time()\n","\n","# Print the minimum, maximum, and average page sizes using Spark loops\n","print(\"using spark loops: \\n\")\n","print(\"Minimum page size (Spark loops):\", min_size_spark)\n","print(\"Maximum page size (Spark loops):\", max_size_spark)\n","print(\"Average page size (Spark loops):\", avg_size_spark)\n","print(\"time taken:\" , end_time - start_time)\n","\n","with open(\"output 1.txt\", \"a\") as f2:\n","  print(\"using spark loops: \\n \\n\",\"Minimum page size (Spark loops):\", min_size_spark ,\"\\n\", \"Maximum page size (Spark loops):\", max_size_spark ,\"\\n\", \"Average page size (Spark loops):\", avg_size_spark ,\"\\n\",\"time taken:\" , end_time - start_time ,\"\\n\", file = f2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O_H48hFsiwxK","executionInfo":{"status":"ok","timestamp":1684593889862,"user_tz":-180,"elapsed":35683,"user":{"displayName":"Yasmen Samir","userId":"08881511615820735826"}},"outputId":"3a586f86-27aa-46c7-b467-232baf0a3551"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["using spark loops: \n","\n","Minimum page size (Spark loops): 0\n","Maximum page size (Spark loops): 141180155987\n","Average page size (Spark loops): 132215.6390405622\n","time taken: 35.70821499824524\n"]}]},{"cell_type":"code","source":["# problem 2\n","# Using map reduce\n","\n","map_reduce_start_time = time.time()\n","\n","def mapper(line):\n","    fields = line.split(\" \")\n","    title = fields[1]\n","    language = fields[0] \n","\n","    if(language != (\"en\")):\n","        if(title.startswith(\"The_\")):\n","            return ((\"the\", \"the_not_english\"), (1, 1))\n","        else:\n","            return ((\"the\", \"the_not_english\"), (0, 0))\n","    else:\n","        if(title.startswith(\"The_\")):\n","            return ((\"the\", \"the_not_english\"), (1, 0))\n","        else:\n","            return ((\"the\", \"the_not_english\"), (0, 0))\n","\n","def reducer(x, y):  # x = (1,1)  -----> y = (1,1)    x[0]=\"the\" in the first key,  y[0]= \"the\" (in the next key)         \n","    return (x[0] + y[0], x[1] + y[1])              # x[1]= \"the_not_english\" in the first key      y[1]= \"the_not_english\" in the next key\n","\n","mapped = lines.map(mapper)\n","counts = mapped.reduceByKey(reducer).collectAsMap() # counts = {(\"The_\",\"the_not_english\"),(41901,9160)}\n","result = list(counts[(\"the\", \"the_not_english\")])  # result = [41901,9160]\n","\n","for i in range(1000000):\n","    pass\n","\n","# stop measuring the execution time\n","map_reduce_end_time = time.time()\n","map_reduce_time = map_reduce_end_time - map_reduce_start_time\n","\n","print(\"Using map-reduce: \\n \\n\", \"Total number of page titles that start with 'The':\", result[0],\"\\n\",\n","      \"Number of non-English page titles that start with 'The':\", result[1],\"\\n\",\n","      \"map reduce Execution time:\", map_reduce_time, \"seconds\")\n","\n","with open(\"output 2.txt\", \"w\") as f:\n","  print(\"Using map-reduce: \\n \\n\", \"Total number of page titles that start with 'The':\", result[0],\"\\n\",\n","      \"Number of non-English page titles that start with 'The':\", result[1],\"\\n\",\n","      \"map reduce Execution time:\", map_reduce_time, \"seconds\", \"\\n\", file=f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tN6m8GMavF7c","executionInfo":{"status":"ok","timestamp":1684725862579,"user_tz":-180,"elapsed":6962,"user":{"displayName":"Yasmen Samir","userId":"08881511615820735826"}},"outputId":"5dbf7f87-f5f6-4636-8241-e4cabf7ab754"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using map-reduce: \n"," \n"," Total number of page titles that start with 'The': 41901 \n"," Number of non-English page titles that start with 'The': 9160 \n"," map reduce Execution time: 6.638685703277588 seconds\n"]}]},{"cell_type":"code","source":["# problem 2\n","# Using Spark loops\n","\n","loops_start_time = time.time()\n","\n","num_of_the_pages = 0\n","num_of_the_non_english_pages = 0\n","\n","for line in lines.toLocalIterator():\n","    # check if the page title starts with \"The\"\n","    fields = line.split(\" \")\n","    if (fields[1].startswith(\"The_\")):\n","        num_of_the_pages += 1\n","        # check if the page is not part of the English project\n","        if (fields[0] != \"en\"):\n","            num_of_the_non_english_pages += 1\n","\n","for i in range(1000000):\n","    pass\n","\n","# stop measuring the execution time\n","loops_end_time = time.time()\n","loops_time = loops_end_time - loops_start_time\n","\n","dic = {loops_time:\"spark loops algorithm\", map_reduce_time:\"map reduce algorithm\"}  # dictionary to store the time of two algorithms\n","best_algo = dic[min(loops_time, map_reduce_time)]\n","\n","print(\"Using Spark loops: \\n \\n\" , \"Total number of page titles that start with 'The':\", num_of_the_pages ,\"\\n\", \n","      \"Number of non-English page titles that start with 'The':\", num_of_the_non_english_pages, \"\\n\",\n","      \"Spark loops Execution time:\", loops_time, \"seconds \\n \\n\",\n","      \"best algorithm in terms of performance:\", best_algo)\n","\n","with open(\"output 2.txt\", \"a\") as f:\n","  print(\"Using Spark loops: \\n \\n\" , \"Total number of page titles that start with 'The':\", num_of_the_pages ,\"\\n\", \n","      \"Number of non-English page titles that start with 'The':\", num_of_the_non_english_pages, \"\\n\",\n","      \"Spark loops Execution time:\", loops_time, \"seconds \\n \\n\",\n","      \"best algorithm in terms of performance:\", best_algo, file=f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GSY1dmQTq-MU","executionInfo":{"status":"ok","timestamp":1684726139089,"user_tz":-180,"elapsed":25449,"user":{"displayName":"Yasmen Samir","userId":"08881511615820735826"}},"outputId":"0f43609d-9883-45e7-cecf-b49f3110ee49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using Spark loops: \n"," \n"," Total number of page titles that start with 'The': 41901 \n"," Number of non-English page titles that start with 'The': 9160 \n"," Spark loops Execution time: 25.064776182174683 seconds \n"," \n"," best algorithm in terms of performance: map reduce algorithm\n"]}]}]}