{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMX3CY54m/g4fHMBFrDU0EK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"Le8YuIQ8sqGU","executionInfo":{"status":"ok","timestamp":1684378964452,"user_tz":-180,"elapsed":11,"user":{"displayName":"Yasmen Samir","userId":"08881511615820735826"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# **Read Data**"],"metadata":{"id":"yt9n5Tnkk3B1"}},{"cell_type":"code","source":["read1 = pd.read_csv('bank_data.csv')\n","df = pd.DataFrame(read1)\n","\n","inputt = input(\"please enter percentage of the data needed to be read in % \\n\").split(\"%\")\n","percentage_of_data = float(inputt[0])\n","nrows = int(len(df) * (percentage_of_data/100))\n","\n","read2 = pd.read_csv('bank_data.csv', nrows=nrows)\n","data = pd.DataFrame(read2, columns=['age','job','marital','education','housing','y'])\n","\n","age_bins = [0, 18, 25, 35, 50, 65, 120]\n","age_labels = ['0-18', '19-25', '26-35', '36-50', '51-65', '65+']\n","\n","# convert age column to categorical column using the age ranges and labels\n","data['age'] = pd.cut(data['age'], bins=age_bins, labels=age_labels)\n","\n","print(data)"],"metadata":{"id":"AgdNoWGnsqXp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684378996380,"user_tz":-180,"elapsed":31932,"user":{"displayName":"Yasmen Samir","userId":"08881511615820735826"}},"outputId":"93297d7e-b102-45b0-942d-2fdcd3a842c3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["please enter percentage of the data needed to be read in % \n","100%\n","        age            job  marital  education housing   y\n","0     26-35     unemployed  married    primary      no  no\n","1     26-35       services  married  secondary     yes  no\n","2     26-35     management   single   tertiary     yes  no\n","3     26-35     management  married   tertiary     yes  no\n","4     51-65    blue-collar  married  secondary     yes  no\n","...     ...            ...      ...        ...     ...  ..\n","4516  26-35       services  married  secondary     yes  no\n","4517  51-65  self-employed  married   tertiary     yes  no\n","4518  51-65     technician  married  secondary      no  no\n","4519  26-35    blue-collar  married  secondary      no  no\n","4520  36-50   entrepreneur   single   tertiary     yes  no\n","\n","[4521 rows x 6 columns]\n"]}]},{"cell_type":"markdown","source":["# **Naive Bayes classifier model**"],"metadata":{"id":"ZEtCxT46kqCR"}},{"cell_type":"code","source":["def calculate_prob_of_lables(df, Y):\n","    classes = sorted(list(df[Y].unique()))\n","    prob_of_lables = []\n","    for i in classes:\n","        prob_of_lables.append(len(df[df[Y]==i])/len(df))\n","    return prob_of_lables"],"metadata":{"id":"QTXEXAi6f4Rm","executionInfo":{"status":"ok","timestamp":1684378996381,"user_tz":-180,"elapsed":13,"user":{"displayName":"Yasmen Samir","userId":"08881511615820735826"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def calculate_prob_of_features(df, feat_name, feat_val, Y, label):\n","    feat = list(df.columns)\n","    df = df[df[Y]==label]\n","    p_x_given_y = len(df[df[feat_name]==feat_val]) / len(df)\n","    return p_x_given_y"],"metadata":{"id":"7F8vm-iFfPlU","executionInfo":{"status":"ok","timestamp":1684378996382,"user_tz":-180,"elapsed":9,"user":{"displayName":"Yasmen Samir","userId":"08881511615820735826"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def NaiveBayesClassifier(df_train, X_test, Y):\n","    # get feature names\n","    features = list(df_train.columns)[:-1]\n","\n","    # calculate probability of lables\n","    prob_of_lables = calculate_prob_of_lables(df_train, Y)\n","\n","    Y_pred = []\n","    # loop over every data sample\n","    for x in X_test:\n","        # calculate probability of features\n","        labels = sorted(list(df_train[Y].unique()))\n","        prob_of_features = [1]*len(labels)\n","        for j in range(len(labels)):  #label length = 2\n","            for i in range(len(features)):  # 4 features\n","                prob_of_features[j] *= calculate_prob_of_features(df_train, features[i], x[i], Y, labels[j])\n","\n","        # calculate the whole probability (p(features) * p(lables))\n","        whole_prob = [1]*len(labels)\n","        for j in range(len(labels)):\n","            whole_prob[j] = prob_of_features[j] * prob_of_lables[j]\n","\n","        if(np.argmax(whole_prob) == 0):\n","          Y_pred.append(\"no\")\n","        else:\n","          Y_pred.append(\"yes\")\n","\n","    return np.array(Y_pred)"],"metadata":{"id":"nqqscWllfXQs","executionInfo":{"status":"ok","timestamp":1684378996382,"user_tz":-180,"elapsed":8,"user":{"displayName":"Yasmen Samir","userId":"08881511615820735826"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# **Test Naive Bayes Model**"],"metadata":{"id":"6kdDWWQ9lDbk"}},{"cell_type":"code","source":["train, test = train_test_split(data, test_size=.2, random_state=1234)\n","\n","X_test = test.iloc[:,:-1].values\n","Y_test = test.iloc[:,-1].values\n","Y_pred = NaiveBayesClassifier(train, X_test, \"y\")\n","\n","NB_accuracy = np.sum(Y_pred == Y_test) / len(Y_pred) * 100\n","print(NB_accuracy , \"%\")"],"metadata":{"id":"lFtLWKWSfYHA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684379010403,"user_tz":-180,"elapsed":14028,"user":{"displayName":"Yasmen Samir","userId":"08881511615820735826"}},"outputId":"61b091bb-3ca2-4bfc-c625-ea4dbfa2f001"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["86.62983425414365 %\n"]}]},{"cell_type":"markdown","source":["# **Decesion Tree classifier model**"],"metadata":{"id":"s-FpwTJTke-k"}},{"cell_type":"code","source":["class Node():\n","    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n","        ''' constructor ''' \n","        \n","        # for decision node\n","        self.feature_index = feature_index\n","        self.threshold = threshold\n","        self.left = left\n","        self.right = right\n","        self.info_gain = info_gain\n","        \n","        # for leaf node\n","        self.value = value"],"metadata":{"id":"8dFtXGgHapxa","executionInfo":{"status":"ok","timestamp":1684379010403,"user_tz":-180,"elapsed":4,"user":{"displayName":"Yasmen Samir","userId":"08881511615820735826"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class DecisionTreeClassifier():\n","    def __init__(self, min_samples_split=2, max_depth=2):\n","        ''' constructor '''\n","        \n","        # initialize the root of the tree \n","        self.root = None\n","        \n","        # stopping conditions\n","        self.min_samples_split = min_samples_split\n","        self.max_depth = max_depth\n","        \n","    def build_tree(self, dataset, curr_depth=0):\n","        ''' recursive function to build the tree ''' \n","        \n","        X, Y = dataset[:,:-1], dataset[:,-1]\n","        num_samples, num_features = np.shape(X)\n","        \n","        # split until stopping conditions are met\n","        if num_samples>=self.min_samples_split and curr_depth<=self.max_depth:\n","            # find the best split\n","            best_split = self.get_best_split(dataset, num_samples, num_features)\n","            # check if information gain is positive\n","            if best_split[\"info_gain\"]>0:\n","                # recur left\n","                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n","                # recur right\n","                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n","                # return decision node\n","                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n","                            left_subtree, right_subtree, best_split[\"info_gain\"])\n","        \n","        # compute leaf node\n","        leaf_value = self.calculate_leaf_value(Y)\n","        # return leaf node\n","        return Node(value=leaf_value)\n","    \n","    def get_best_split(self, dataset, num_samples, num_features):\n","        ''' function to find the best split '''\n","        \n","        # dictionary to store the best split\n","        best_split = {}\n","        max_info_gain = -float(\"inf\")\n","        \n","        # loop over all the features\n","        for feature_index in range(num_features):\n","            feature_values = dataset[:, feature_index]\n","            possible_thresholds = np.unique(feature_values)\n","\n","            #loop over all the feature values present in the data\n","            for threshold in possible_thresholds:\n","                # get current split\n","                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n","                # check if childs are not null\n","                if len(dataset_left)>0 and len(dataset_right)>0:\n","                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n","                    # compute information gain\n","                    curr_info_gain = self.information_gain(y, left_y, right_y)\n","                    # update the best split if needed\n","                    if curr_info_gain>max_info_gain:\n","                        best_split[\"feature_index\"] = feature_index\n","                        best_split[\"threshold\"] = threshold\n","                        best_split[\"dataset_left\"] = dataset_left\n","                        best_split[\"dataset_right\"] = dataset_right\n","                        best_split[\"info_gain\"] = curr_info_gain\n","                        max_info_gain = curr_info_gain\n","                        \n","        return best_split\n","    \n","    def split(self, dataset, feature_index, threshold):\n","        ''' function to split the data '''\n","        \n","        dataset_left = np.array([row for row in dataset if row[feature_index] != threshold]) # row = [['retired' 'married' 'secondary' 'no' 'no']         row[0] = ['retired' 'admin.' 'self-employed' ... 'blue-collar' 'admin.''blue-collar']\n","                                                                                                  #   ['admin.' 'married' 'secondary' 'yes' 'no']]\n","        dataset_right = np.array([row for row in dataset if row[feature_index] == threshold])\n","        return dataset_left, dataset_right\n","    \n","    def information_gain(self, parent, l_child, r_child ,mode=\"entropy\"):\n","        ''' function to compute information gain '''\n","        \n","        weight_l = len(l_child) / len(parent)\n","        weight_r = len(r_child) / len(parent)\n","\n","        if mode==\"gini\":\n","            gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n","        else:\n","            gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))\n","        return gain\n","\n","        return gain\n","    \n","    def entropy(self, y_given_feature):\n","        ''' function to compute entropy '''\n","        \n","        class_labels = np.unique(y_given_feature)  # y_given_feature : job = unemployed is yes ---> r_child or no ---> l_child\n","        entropy = 0\n","        for label in class_labels:\n","            p_label = len(y_given_feature[y_given_feature == label]) / len(y_given_feature)   # l_child = low/no , r_child = low/yes ------> low is (feature value)\n","            entropy += -p_label * np.log2(p_label)\n","        return entropy\n","\n","    def gini_index(self, y_given_feature):\n","        ''' function to compute gini index '''\n","        \n","        class_labels = np.unique(y_given_feature)\n","        gini = 0\n","        for label in class_labels:\n","            p_label = len(y_given_feature[y_given_feature == label]) / len(y_given_feature)\n","            gini += p_label**2\n","        return 1 - gini\n","        \n","    def calculate_leaf_value(self, Y):\n","        ''' function to compute leaf node '''\n","        \n","        Y = list(Y)\n","        return max(Y, key=Y.count)\n","    \n","    def fit(self, X, Y):\n","        ''' function to train the tree '''\n","        \n","        dataset = np.concatenate((X, Y), axis=1)\n","        self.root = self.build_tree(dataset)\n","        return dataset\n","    \n","    def predict(self, X):\n","        ''' function to predict new dataset '''\n","        \n","        predictions = [self.make_prediction(x, self.root) for x in X]\n","        return predictions\n","\n","    def make_prediction(self, x, tree):\n","        ''' function to predict a single data point '''\n","        \n","        if tree.value!=None: return tree.value\n","\n","        feature_val = x[tree.feature_index]\n","        if feature_val == tree.threshold:\n","            return self.make_prediction(x, tree.right)\n","        else:\n","            return self.make_prediction(x, tree.left)\n","            "],"metadata":{"id":"ORdHAk4Bcv9f","executionInfo":{"status":"ok","timestamp":1684379010403,"user_tz":-180,"elapsed":3,"user":{"displayName":"Yasmen Samir","userId":"08881511615820735826"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# **Test Decesion Tree Model**"],"metadata":{"id":"a96roVGrlJhE"}},{"cell_type":"code","source":["inputData = data.iloc[:, :-1].values\n","outputData = data.iloc[:, -1].values.reshape(-1,1)\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(inputData, outputData, test_size=.2, random_state=1234)\n","\n","classifier = DecisionTreeClassifier(min_samples_split=3, max_depth=3)\n","classifier.fit(X_train,Y_train)\n","\n","Y_pred = classifier.predict(X_test) \n","Y_pred = np.array(Y_pred).reshape(-1, 1)\n","\n","DT_accuracy = np.sum(Y_pred == Y_test) / len(Y_test) * 100\n","\n","print(DT_accuracy , \"%\")"],"metadata":{"id":"rotFBbf3d2VS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684379011110,"user_tz":-180,"elapsed":710,"user":{"displayName":"Yasmen Samir","userId":"08881511615820735826"}},"outputId":"8f76b489-3a3e-4f1f-84f2-38113739eb37"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["87.18232044198895 %\n"]}]},{"cell_type":"markdown","source":["# **Two classifiers accuracies comparison**"],"metadata":{"id":"l1D5spPjiECy"}},{"cell_type":"code","source":["print(\"Decesion tree classifier accuracy =\", DT_accuracy, \"%\"+ \"\\n\")\n","print(\"Naive Bayes classifier accuracy =\", NB_accuracy, \"%\" + \"\\n\")\n","\n","if(NB_accuracy > DT_accuracy):\n","  print(\"Naive Bayes classifier is more accurate than Decesion tree classifier\")\n","elif(NB_accuracy < DT_accuracy):\n","   print(\"Decesion tree classifier is more accurate than Naive Bayes classifier\")\n","else:\n","  print(\"Both classifiers have the same accuracy\")"],"metadata":{"id":"c7oxOfTviCjj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684379011110,"user_tz":-180,"elapsed":5,"user":{"displayName":"Yasmen Samir","userId":"08881511615820735826"}},"outputId":"067cb934-5dfd-42a9-9d14-231a043795a8"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Decesion tree classifier accuracy = 87.18232044198895 %\n","\n","Naive Bayes classifier accuracy = 86.62983425414365 %\n","\n","Decesion tree classifier is more accurate than Naive Bayes classifier\n"]}]}]}